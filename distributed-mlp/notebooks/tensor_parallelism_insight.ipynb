{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT0zxYb8qYH5"
      },
      "source": [
        "# 1. Dataset and Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnb3QJmfYbzk"
      },
      "source": [
        "## 1.1. Random Dataset\n",
        "This randomly generate the input and output data with given sizes and also the number of samples.\n",
        "\n",
        "It uses `torch.manual_seed` to generate same data in each run for consistensy\n",
        "\n",
        "A data loader then uses this dataset and create the data batches with given batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FnInplIWJI0o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class RandomTensorDataset(Dataset):\n",
        "  def __init__(self, num_samples, in_shape, out_shape):\n",
        "    self.num_samples = num_samples\n",
        "    torch.manual_seed(12345)\n",
        "    self.data = [(torch.randn(in_shape), torch.randn(out_shape)) for _ in range(num_samples)]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.num_samples\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx]\n",
        "\n",
        "input_size  = 6\n",
        "output_size = 2\n",
        "\n",
        "# dataset construction\n",
        "num_samples = 32\n",
        "dataset = RandomTensorDataset(\n",
        "  num_samples=num_samples,\n",
        "  in_shape=input_size,\n",
        "  out_shape=output_size\n",
        "  )\n",
        "\n",
        "batch_size  = 32 # One batch in total since the total number of data samples are 32\n",
        "dataloader = DataLoader(\n",
        "  dataset,\n",
        "  batch_size=batch_size,\n",
        "  pin_memory=True,\n",
        "  shuffle=False\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYdfoU_0ZqQ_"
      },
      "source": [
        "## 1.2. A Simple 2-Layer MLP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OmmhpG7ZT9Fi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, in_feature, hidden_units, out_feature):\n",
        "    super().__init__()\n",
        "    torch.manual_seed(12345)\n",
        "    self.hidden_layer = nn.Linear(in_feature, hidden_units)\n",
        "    self.output_layer = nn.Linear(hidden_units, out_feature)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.hidden_layer(x)\n",
        "    x = self.output_layer(x)\n",
        "    return x\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Using single GPU (GPU 0) if available otherwise CPU\n",
        "\n",
        "# model construction\n",
        "layer_1_units = input_size\n",
        "layer_2_units = 4\n",
        "layer_3_units = output_size\n",
        "model = MLP(\n",
        "  in_feature=layer_1_units,\n",
        "  hidden_units=layer_2_units,\n",
        "  out_feature=layer_3_units\n",
        "  ).to(device)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LHF-bsbbLap"
      },
      "source": [
        "## 1.3. Extracting Each Layer Original Weights and Biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj6EfWAkbFUj",
        "outputId": "bece03e0-dc22-42eb-cce5-77bf197ab9ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W1.shape: torch.Size([4, 6]),  b1.shape: torch.Size([4])\n",
            "W2.shape: torch.Size([2, 4]) b2.shape: torch.Size([2])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# hidden layer parameters\n",
        "W1 = model.hidden_layer.weight\n",
        "b1 = model.hidden_layer.bias\n",
        "\n",
        "# output layer parameters\n",
        "W2 = model.output_layer.weight\n",
        "b2 = model.output_layer.bias\n",
        "\n",
        "print(f\"W1.shape: {W1.shape},  b1.shape: {b1.shape}\\nW2.shape: {W2.shape} b2.shape: {b2.shape}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5J8QjPetc6R"
      },
      "source": [
        "# 2. Run One Epoch Using PyTorch On Single Device\n",
        "\n",
        "We dont update the Model Parameters yet since we need them in the next section to compute forward pass and backward pass manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDubt4cdFs8-",
        "outputId": "d0946a13-ead0-426e-db6f-1547d7cc222f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda For One Iteration of Forward and Backward Passes Using PyTorch\n",
            "out=tensor([[ 0.1546, -0.5383],\n",
            "        [ 0.1222, -0.3119],\n",
            "        [ 0.6736, -0.2439],\n",
            "        [ 0.8124, -0.3054],\n",
            "        [ 0.4496, -0.4473],\n",
            "        [ 0.4101, -0.5774],\n",
            "        [ 0.4743, -0.6111],\n",
            "        [ 0.5034, -0.3675],\n",
            "        [ 0.3844, -0.3518],\n",
            "        [ 0.5095, -0.5411],\n",
            "        [ 0.6302, -0.4292],\n",
            "        [ 0.0960, -0.5650],\n",
            "        [ 0.5374, -0.3396],\n",
            "        [ 0.4914, -0.3475],\n",
            "        [ 0.9440, -0.1130],\n",
            "        [ 0.7744, -0.2111],\n",
            "        [ 0.4808, -0.3415],\n",
            "        [ 0.3199, -0.5394],\n",
            "        [ 0.5131, -0.4608],\n",
            "        [-0.0191, -0.6592],\n",
            "        [ 0.3838, -0.4386],\n",
            "        [ 0.5702, -0.2098],\n",
            "        [ 0.2020, -0.4805],\n",
            "        [ 0.1737, -0.2273],\n",
            "        [ 0.6647, -0.2574],\n",
            "        [ 0.6794, -0.2492],\n",
            "        [ 0.6364, -0.3637],\n",
            "        [ 1.0508, -0.2086],\n",
            "        [ 0.5117, -0.4432],\n",
            "        [ 0.4517, -0.2172],\n",
            "        [ 0.5502, -0.5689],\n",
            "        [ 0.6571, -0.0120]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# One iteration using PyTorch\n",
        "print(f'Using {device} For One Iteration of Forward and Backward Passes Using PyTorch')\n",
        "for x, y in dataloader:\n",
        "  x = x.to(device)\n",
        "  y = y.to(device)\n",
        "\n",
        "  # Forward Pass\n",
        "  out = model(x)\n",
        "\n",
        "  # Calculate loss\n",
        "  loss = loss_fn(out, y)\n",
        "\n",
        "  # Zero grad\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "  # Backward Pass\n",
        "  loss.backward()\n",
        "\n",
        "  # We update the model on Section 4. (`optimizer.step()`)\n",
        "\n",
        "print(f'{out=}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaYBhh_6rBwE"
      },
      "source": [
        "# 3. Manual Computation\n",
        "* Simulate Forward Pass on a single device\n",
        "* Simulate Tensor Parallelism (TP) of Forward Pass on two devices\n",
        "* Simulate Backward Pass on a single device\n",
        "* Simulate Tensor Parallelism (TP) of Backward Pass on two devices\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFCcohC4Zkb7"
      },
      "source": [
        "First let's have a function to compare two tensors for equality.\n",
        "\n",
        "Here is a function to compare two tensor if they are the same.\n",
        "\n",
        "Note since we are doing Floating Point operations it is acceptable if they are not exactly the same but very close."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CQgTXOOkaDtN"
      },
      "outputs": [],
      "source": [
        "def cmp(s, t1, t2):\n",
        "  ex = torch.all(t1 == t2).item()\n",
        "  app = torch.allclose(t1, t2)\n",
        "  maxdiff = (t1 - t2).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ9inWynZt4Q"
      },
      "source": [
        "## 3.1. Forward Pass\n",
        "Doing the forward pass manually and compare with what PyTorch has given to us in previous section (out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSdYJkocaj1M"
      },
      "source": [
        "### 3.1.1 Simulate Forward Pass On One Device\n",
        "You can see that the manual output is exactly match the model output generated with pytorch on the previous section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiE3w9K2qOD-",
        "outputId": "e465f724-0f0b-4166-cc27-9132b6bf4e0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "forward pass    | exact: True  | approximate: True  | maxdiff: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Forward Pass Manually\n",
        "h = x @ W1.T + b1\n",
        "manual_out = h @ W2.T + b2\n",
        "cmp('forward pass', manual_out, out) # is it consistent with pytorch?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kVL5TK9aZf_"
      },
      "source": [
        "### 3.1.2 Simulate Forward Pass On Two Devices Using Tensor Parallelism (TP)\n",
        "Using TP to distribute the computation across two devices, we assume that each layer parameters are sharded column-wise between two devices and perform the forward computation manually to understand how TP works and where it needs communication.\n",
        "\n",
        "lets first see how the model parameters are divided between the two devices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vECoZ6OD_cTZ",
        "outputId": "9e8470f5-8b38-4454-8cc4-4133ead49588"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0 : W1.shape: torch.Size([2, 6]),  b1.shape: torch.Size([2])\n",
            "W2.shape: torch.Size([1, 4]) b2.shape: torch.Size([1])\n",
            "\n",
            "GPU 0 : W1.shape: torch.Size([2, 6]),  b1.shape: torch.Size([2])\n",
            "W2.shape: torch.Size([1, 4]) b2.shape: torch.Size([1])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "############## on CPU 0\n",
        "  # Layer 1\n",
        "W1_0 = W1[:2]\n",
        "b1_0 = b1[:2]\n",
        "  # Layer 2\n",
        "W2_0 = W2[:1]\n",
        "b2_0 = b2[:1]\n",
        "print(f\"GPU 0 : W1.shape: {W1_0.shape},  b1.shape: {b1_0.shape}\\nW2.shape: {W2_0.shape} b2.shape: {b2_0.shape}\\n\")\n",
        "\n",
        "############## on CPU 1\n",
        "  # Layer 1\n",
        "W1_1 = W1[2:]\n",
        "b1_1 = b1[2:]\n",
        "  # Layer 2\n",
        "W2_1 = W2[1:]\n",
        "b2_1 = b2[1:]\n",
        "print(f\"GPU 0 : W1.shape: {W1_1.shape},  b1.shape: {b1_1.shape}\\nW2.shape: {W2_1.shape} b2.shape: {b2_1.shape}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLYziK--euZs"
      },
      "source": [
        "Now we compute the forward pass for each device locally (independently) and after all_gather we compare it with the pytorch output.\n",
        "\n",
        "The aggregated output match the output we got from PyTorch forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0ok7xCDe9r1",
        "outputId": "938c5801-2180-48fd-d53d-728be5619ec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TP - forward pass | exact: False | approximate: True  | maxdiff: 5.960464477539063e-08\n"
          ]
        }
      ],
      "source": [
        "# GPU 0\n",
        "h_0 = x @ W1_0.T + b1_0\n",
        "# GPU 1\n",
        "h_1 = x @ W1_1.T + b1_1\n",
        "\n",
        "# All_gather h before starting the forward pass for the next layer\n",
        "gathered_h = torch.cat((h_0, h_1), dim=1)\n",
        "\n",
        "# GPU 0\n",
        "manual_out_0 = gathered_h @ W2_0.T + b2_0\n",
        "# GPU 1\n",
        "manual_out_1 = gathered_h @ W2_1.T + b2_1\n",
        "\n",
        "# All_gather output\n",
        "gathered_manual_out = torch.cat((manual_out_0, manual_out_1), dim=1)\n",
        "\n",
        "cmp('TP - forward pass', gathered_manual_out, out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xFwXYUKv7Gj"
      },
      "source": [
        "## 3.2 Backward Pass\n",
        "Now lets do the backward pass manually and calculate the gradients and compare it with what loss.backward() computes by PyTorch in Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlRtXBlRw7YQ"
      },
      "source": [
        "### 3.2.1 Simulate Backward Pass On One Device\n",
        "* Following if use d{variable} meaning dL/d{variable}\n",
        "* We use unsqueeze to add a dimension with size 1 in order to use broadcasting for element-wise multiplication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QuRyfQnwhQR",
        "outputId": "27dbdaaa-bf12-41c8-b67a-33c0a2e710de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dW2             | exact: False | approximate: True  | maxdiff: 2.9802322387695312e-08\n",
            "db2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "dW1             | exact: False | approximate: True  | maxdiff: 2.9802322387695312e-08\n",
            "db1             | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n"
          ]
        }
      ],
      "source": [
        "# Backward Pass Manually\n",
        "dout = (manual_out - y)\n",
        "\n",
        "# Output Layer\n",
        "dW2 = (dout.unsqueeze(2) * h.unsqueeze(1)).mean(dim=0) # Avarage across 32 data points in the batch\n",
        "db2 = dout.mean(dim=0)\n",
        "# Comparing with PyTorch gradients\n",
        "cmp('dW2', dW2, W2.grad)\n",
        "cmp('db2', db2, b2.grad)\n",
        "\n",
        "dh = (dout.unsqueeze(2) * W2.unsqueeze(0)).sum(1)\n",
        "\n",
        "# Hidden Layer\n",
        "dW1 = (dh.unsqueeze(2) * x.unsqueeze(1)).mean(dim=0)\n",
        "db1 = dh.mean(0)\n",
        "cmp('dW1', dW1, W1.grad)\n",
        "cmp('db1', db1, b1.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQKSm-5h55uk"
      },
      "source": [
        "### 3.2.2 Simulate Backward Pass On Two Devices Using Tensor Parallelism (TP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpL7JSqJ3LAa",
        "outputId": "9a2f3b5a-3c63-4077-b8b4-f310c445d874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dW2_0           | exact: False | approximate: True  | maxdiff: 2.9802322387695312e-08\n",
            "db2_0           | exact: False | approximate: True  | maxdiff: 5.960464477539063e-08\n",
            "dW2_1           | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
            "db2_1           | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
            "dW1_0           | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
            "db1_0           | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "dW1_1           | exact: False | approximate: True  | maxdiff: 2.9802322387695312e-08\n",
            "db1_1           | exact: True  | approximate: True  | maxdiff: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Output Layer\n",
        "# GPU 0\n",
        "dout_0 = (manual_out_0 - y[:, :1])\n",
        "dW2_0 = (dout_0.unsqueeze(2) * gathered_h.unsqueeze(1)).mean(0)\n",
        "db2_0 = dout_0.mean(0)\n",
        "cmp('dW2_0', dW2_0, W2.grad[:1])\n",
        "cmp('db2_0', db2_0, b2.grad[:1])\n",
        "\n",
        "# GPU 1\n",
        "dout_1 = (manual_out_1 - y[:, 1:])\n",
        "dW2_1 = (dout_1.unsqueeze(2) * gathered_h.unsqueeze(1)).mean(0)\n",
        "db2_1 = dout_1.mean(0)\n",
        "cmp('dW2_1', dW2_1, W2.grad[1:])\n",
        "cmp('db2_1', db2_1, b2.grad[1:])\n",
        "###########\n",
        "\n",
        "dh_0 = (dout_0.unsqueeze(2) * W2_0.unsqueeze(0)).sum(1)\n",
        "dh_1 = (dout_1.unsqueeze(2) * W2_1.unsqueeze(0)).sum(1)\n",
        "# Reduce-scatter h\n",
        "reduce_scatter_dh_0 = (dh_0 + dh_1)[:, :2]\n",
        "reduce_scatter_dh_1 = (dh_0 + dh_1)[:, 2:]\n",
        "\n",
        "###########\n",
        "# Hidden Layer\n",
        "# GPU 0\n",
        "dW1_0 = (reduce_scatter_dh_0.unsqueeze(2) * x.unsqueeze(1)).mean(0)\n",
        "db1_0 = reduce_scatter_dh_0.mean(0)\n",
        "cmp('dW1_0', dW1_0, W1.grad[:2])\n",
        "cmp('db1_0', db1_0, b1.grad[:2])\n",
        "\n",
        "# GPU 1\n",
        "dW1_1 = (reduce_scatter_dh_1.unsqueeze(2) * x.unsqueeze(1)).mean(0)\n",
        "db1_1 = reduce_scatter_dh_1.mean(0)\n",
        "cmp('dW1_1', dW1_1, W1.grad[2:])\n",
        "cmp('db1_1', db1_1, b1.grad[2:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsSBurWvMwcQ"
      },
      "source": [
        "# 4. Update The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKNd2FPPNdqg"
      },
      "source": [
        "## 4.2 Manually\n",
        "Using gradients calculated in Section 3 to update the model manually and compare it with SGD function from PyTorch.\n",
        "\n",
        "We first compute the manual update since we need the current model parameters. If we run `optimizer.step()` first then the model parameters change and make the manual computation incorrect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NLQlv_tqpo6l"
      },
      "outputs": [],
      "source": [
        "# Using the same learning rate as Section 1\n",
        "lr = 0.01\n",
        "manual_nW1 = W1 - lr * dW1\n",
        "manual_nb1 = b1 - lr * db1\n",
        "manual_nW2 = W2 - lr * dW2\n",
        "manual_nb2 = b2 - lr * db2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90evr9I-M50p"
      },
      "source": [
        "## 4.1 PyTorch\n",
        "Now we run `optimizer.step()` to update the model then compare the updated parameters with the manually computed ones from previous subsection.\n",
        "\n",
        "Note that W1, b1, W2, b2 still point the updated mpdel parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ukSI7-lbOFo-"
      },
      "outputs": [],
      "source": [
        "# Run optimizer.step() to update the model\n",
        "optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25lcSAVFz14r",
        "outputId": "0d91dc9b-af51-43b2-9b1b-075cc74acec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "manual_nW1      | exact: False | approximate: True  | maxdiff: 2.9802322387695312e-08\n",
            "manual_nb1      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "manual_nW2      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "manual_nb2      | exact: True  | approximate: True  | maxdiff: 0.0\n"
          ]
        }
      ],
      "source": [
        "cmp('manual_nW1', manual_nW1, W1)\n",
        "cmp('manual_nb1', manual_nb1, b1)\n",
        "cmp('manual_nW2', manual_nW2, W2)\n",
        "cmp('manual_nb2', manual_nb2, b2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
