{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT0zxYb8qYH5"
      },
      "source": [
        "# 1. Dataset and Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnb3QJmfYbzk"
      },
      "source": [
        "## 1.1. Random Dataset\n",
        "This randomly generate the input and output data with given sizes and also the number of samples.\n",
        "\n",
        "It uses `torch.manual_seed` to generate same data for consistensy on different experiments\n",
        "\n",
        "A data loader then uses this dataset and create the data batches with given batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "FnInplIWJI0o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class RandomTensorDataset(Dataset):\n",
        "  def __init__(self, num_samples, in_shape, out_shape):\n",
        "    self.num_samples = num_samples\n",
        "    torch.manual_seed(12345)\n",
        "    self.data = [(torch.randn(in_shape), torch.randn(out_shape)) for _ in range(num_samples)]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.num_samples\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx]\n",
        "\n",
        "input_size  = 6\n",
        "output_size = 2\n",
        "\n",
        "# dataset construction\n",
        "num_samples = 64\n",
        "dataset = RandomTensorDataset(\n",
        "  num_samples=num_samples,\n",
        "  in_shape=input_size,\n",
        "  out_shape=output_size\n",
        "  )\n",
        "\n",
        "batch_size  = 32 # two batches\n",
        "dataloader = DataLoader(\n",
        "  dataset,\n",
        "  batch_size=batch_size,\n",
        "  pin_memory=True,\n",
        "  shuffle=False\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYdfoU_0ZqQ_"
      },
      "source": [
        "## 1.2. A Simple 2-Layer MLP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "OmmhpG7ZT9Fi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, in_feature, hidden_units, out_feature):\n",
        "    super().__init__()\n",
        "    torch.manual_seed(12345)\n",
        "    self.hidden_layer = nn.Linear(in_feature, hidden_units)\n",
        "    self.output_layer = nn.Linear(hidden_units, out_feature)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.hidden_layer(x)\n",
        "    x = self.output_layer(x)\n",
        "    return x\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Using single GPU (GPU 0) if available otherwise CPU\n",
        "\n",
        "# model construction\n",
        "layer_1_units = input_size\n",
        "layer_2_units = 4\n",
        "layer_3_units = output_size\n",
        "model = MLP(\n",
        "  in_feature=layer_1_units,\n",
        "  hidden_units=layer_2_units,\n",
        "  out_feature=layer_3_units\n",
        "  ).to(device)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5J8QjPetc6R"
      },
      "source": [
        "# 2. Run One Epoch On A Single Device\n",
        "The single device will train the model on the both data batches. Later we simulate the DDP behaviour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDubt4cdFs8-",
        "outputId": "d0946a13-ead0-426e-db6f-1547d7cc222f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda For One Iteration of Forward and Backward Passes Using PyTorch\n",
            "epoch 1 | train_loss=tensor(2.1911, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# One iteration using PyTorch\n",
        "print(f'Using {device} For One Iteration of Forward and Backward Passes Using PyTorch')\n",
        "train_loss = 0\n",
        "for x, y in dataloader:\n",
        "  x = x.to(device)\n",
        "  y = y.to(device)\n",
        "\n",
        "  # Forward Pass\n",
        "  out = model(x)\n",
        "\n",
        "  # Calculate loss\n",
        "  loss = loss_fn(out, y)\n",
        "  train_loss += loss\n",
        "\n",
        "  # Zero grad\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "  # Backward Pass\n",
        "  loss.backward()\n",
        "\n",
        "  # Update Model\n",
        "  optimizer.step()\n",
        "\n",
        "print(f'epoch 1 | {train_loss=}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaYBhh_6rBwE"
      },
      "source": [
        "# 3. Simulating DDP on Two Devices\n",
        "* Divide the dataset into two separate parts (one batch each)\n",
        "* Create two instances of the model, one for each device\n",
        "* Train each model replica on the separate data batches\n",
        "* All-reduce the gradient\n",
        "* Update the model replicas.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ9inWynZt4Q"
      },
      "source": [
        "## 3.1 Simulate Dividing the Dataset in DDP \n",
        "Each device gets half of the dataset (one batch each in this case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_0.shape: torch.Size([32, 6]), y_0.shape: torch.Size([32, 2])\n",
            "x_1.shape: torch.Size([32, 6]), y_1.shape: torch.Size([32, 2])\n"
          ]
        }
      ],
      "source": [
        "iterator = iter(dataloader)\n",
        "##### Device 0 (GPU 0)\n",
        "x_0, y_0 = next(iterator)\n",
        "\n",
        "##### Device 1 (GPU 1)\n",
        "x_1, y_1 = next(iterator)\n",
        "\n",
        "print(f'x_0.shape: {x_0.shape}, y_0.shape: {y_0.shape}\\nx_1.shape: {x_1.shape}, y_1.shape: {y_1.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2. Copy The Model For Each Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "####################################### Device 0 (GPU 0)\n",
        "ddp_model_0 = MLP(\n",
        "  in_feature=layer_1_units,\n",
        "  hidden_units=layer_2_units,\n",
        "  out_feature=layer_3_units\n",
        "  ).to(device)\n",
        "optimizer_0 = optim.SGD(ddp_model_0.parameters(),lr=0.01)\n",
        "\n",
        "####################################### Device 1 (GPU 1)\n",
        "ddp_model_1 = MLP(\n",
        "  in_feature=layer_1_units,\n",
        "  hidden_units=layer_2_units,\n",
        "  out_feature=layer_3_units\n",
        "  ).to(device)\n",
        "optimizer_1 = optim.SGD(ddp_model_1.parameters(),lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3. Simulate the DDP Forward and Backward Computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "####################################### Device 0 (GPU 0)\n",
        "x_0 = x_0.to(device)\n",
        "y_0 = y_0.to(device)\n",
        "\n",
        "# Forward Pass\n",
        "out_0 = ddp_model_0(x_0)\n",
        "\n",
        "# Calculate loss\n",
        "loss_0 = loss_fn(out_0, y_0)\n",
        "\n",
        "# Zero grad\n",
        "optimizer_0.zero_grad(set_to_none=True)\n",
        "\n",
        "# Backward Pass\n",
        "loss_0.backward() # local gradients\n",
        "\n",
        "\n",
        "####################################### Device 1 (GPU 1)\n",
        "x_1 = x_1.to(device)\n",
        "y_1 = y_1.to(device)\n",
        "\n",
        "# Forward Pass\n",
        "out_1 = ddp_model_1(x_1)\n",
        "\n",
        "# Calculate loss\n",
        "loss_1 = loss_fn(out_1, y_1)\n",
        "\n",
        "# Zero grad\n",
        "optimizer_1.zero_grad(set_to_none=True)\n",
        "\n",
        "# Backward Pass\n",
        "loss_1.backward() # local gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsSBurWvMwcQ"
      },
      "source": [
        "# 3.4. Simulate DDP Gradients All-Reduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "####################################### Device 0 (GPU 0)\n",
        "# hidden layer parameters\n",
        "W1_0 = ddp_model_0.hidden_layer.weight\n",
        "b1_0 = ddp_model_0.hidden_layer.bias\n",
        "\n",
        "# output layer parameters\n",
        "W2_0 = ddp_model_0.output_layer.weight\n",
        "b2_0 = ddp_model_0.output_layer.bias\n",
        "\n",
        "####################################### Device 1 (GPU 1)\n",
        "# hidden layer parameters\n",
        "W1_1 = ddp_model_1.hidden_layer.weight\n",
        "b1_1 = ddp_model_1.hidden_layer.bias\n",
        "\n",
        "# output layer parameters\n",
        "W2_1 = ddp_model_1.output_layer.weight\n",
        "b2_1 = ddp_model_1.output_layer.bias\n",
        "\n",
        "###################################### All-Reduce Gradients\n",
        "W1_0.grad = (W1_0.grad + W1_1.grad)/2\n",
        "W1_1.grad = W1_0.grad\n",
        "b1_0.grad = (b1_0.grad + b1_1.grad)/2\n",
        "b1_1.grad = b1_0.grad\n",
        "W2_0.grad = (W2_0.grad + W2_1.grad)/2\n",
        "W2_1.grad = W2_0.grad\n",
        "b2_0.grad = (b2_0.grad + b2_1.grad)/2\n",
        "b2_1.grad = b2_0.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.5. Update The Model Replicas on Each Device\n",
        "Update each model replicas and compare them to make sure model replicas are consistent across the two devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update each model replicas\n",
        "optimizer_0.step()\n",
        "optimizer_1.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Function to compare two tensors\n",
        "def cmp(s, t1, t2):\n",
        "  ex = torch.all(t1 == t2).item()\n",
        "  app = torch.allclose(t1, t2)\n",
        "  maxdiff = (t1 - t2).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n",
        "\n",
        "cmp('W1', W1_0, W1_1)\n",
        "cmp('b1', b1_0, b1_1)\n",
        "cmp('W2', W2_0, W2_1)\n",
        "cmp('b2', b2_0, b2_1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
