In this example, we take a simple MLP network and illustrates the insight of different distributed training approaches.

Following is the mlp network diagram and its corrsponding parameters (weight and bias matrices).

| 2-layer MLP | Corresponding Matrices |
|:-----:|:-----:|
| <img src="figures/mlp_network.png" style="width: 60%;"/>| <img src="figures/mlp_matrices.png"/> |

# Environment Setup On HPC Cluster
## 1. Create conda environment
Creating the conda envireonment named `dist_computing` (one can use their own customized name).
```{code} bash
conda create -n dist_computing python=3.10
```
## 2. Installing PyTorch
```{code} bash
# Activating the conda environment and install PyTorch:
conda activate dist_computing
pip3 install torch
```

# Run On HPC Cluster
[scripts](scripts/) directory contains mlp scripts to run single GPU as well as multi-GPU scripts for Data, Model and Tensor Parallelism. It also slurm script skeletons that you can use to run the exmaples after creating the codna environment as described.

For example, you can use the [slurm_single_gpu.sh](scripts/slurm_single_gpu.sh) to submit a job on the cluster to train the above MLP network for 1 epoch on a single GPU. It uses 32 batches of random data generated by [random_dataset](scripts/random_dataset.py).